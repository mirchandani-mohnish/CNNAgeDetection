{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":263465,"sourceType":"datasetVersion","datasetId":110215}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, mean_squared_error\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:59:07.771813Z","iopub.execute_input":"2024-12-15T19:59:07.772060Z","iopub.status.idle":"2024-12-15T19:59:14.876640Z","shell.execute_reply.started":"2024-12-15T19:59:07.772035Z","shell.execute_reply":"2024-12-15T19:59:14.875722Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nbase_path = '/kaggle/input/facial-age/face_age/'\nimage_folders = os.listdir(base_path)\n\n# print(image_folders)\n\nage_labels = []\ngender_labels = []\nimage_paths = []\nimage_names = []\n\nfor dir_name in image_folders:\n    dir_path = os.path.join(base_path, dir_name)\n    image_files = os.listdir(dir_path)\n    for img in image_files:\n        try:\n            age = int(dir_name)\n            age_labels.append(age)\n            image_names.append(img)\n            image_paths.append(os.path.join(base_path,dir_name,img))\n        except Exception as e:\n            # the dataset folder has another folder with the dataset itself: This will prevent that folder from joining the df\n            continue\n            \n\ndf = pd.DataFrame({\n    'image_path': image_paths,\n    'image_name': image_names,\n    'age': age_labels\n})\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:59:14.878430Z","iopub.execute_input":"2024-12-15T19:59:14.878812Z","iopub.status.idle":"2024-12-15T19:59:15.458814Z","shell.execute_reply.started":"2024-12-15T19:59:14.878785Z","shell.execute_reply":"2024-12-15T19:59:15.458002Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                          image_path image_name  age\n0     /kaggle/input/facial-age/face_age/057/6802.png   6802.png   57\n1     /kaggle/input/facial-age/face_age/057/3702.png   3702.png   57\n2     /kaggle/input/facial-age/face_age/057/8810.png   8810.png   57\n3     /kaggle/input/facial-age/face_age/057/6759.png   6759.png   57\n4     /kaggle/input/facial-age/face_age/057/1846.png   1846.png   57\n...                                              ...        ...  ...\n9773  /kaggle/input/facial-age/face_age/085/7733.png   7733.png   85\n9774  /kaggle/input/facial-age/face_age/085/3123.png   3123.png   85\n9775  /kaggle/input/facial-age/face_age/085/5302.png   5302.png   85\n9776  /kaggle/input/facial-age/face_age/085/9632.png   9632.png   85\n9777  /kaggle/input/facial-age/face_age/085/1710.png   1710.png   85\n\n[9778 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>image_name</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/facial-age/face_age/057/6802.png</td>\n      <td>6802.png</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/facial-age/face_age/057/3702.png</td>\n      <td>3702.png</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/facial-age/face_age/057/8810.png</td>\n      <td>8810.png</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/facial-age/face_age/057/6759.png</td>\n      <td>6759.png</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/facial-age/face_age/057/1846.png</td>\n      <td>1846.png</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9773</th>\n      <td>/kaggle/input/facial-age/face_age/085/7733.png</td>\n      <td>7733.png</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>9774</th>\n      <td>/kaggle/input/facial-age/face_age/085/3123.png</td>\n      <td>3123.png</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>9775</th>\n      <td>/kaggle/input/facial-age/face_age/085/5302.png</td>\n      <td>5302.png</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>9776</th>\n      <td>/kaggle/input/facial-age/face_age/085/9632.png</td>\n      <td>9632.png</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>9777</th>\n      <td>/kaggle/input/facial-age/face_age/085/1710.png</td>\n      <td>1710.png</td>\n      <td>85</td>\n    </tr>\n  </tbody>\n</table>\n<p>9778 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df['Age_Group'] = pd.qcut(df['age'], q=4, labels=range(0,4))\ndf['Age_Interval'] = pd.qcut(df['age'], q=4)\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:59:15.459784Z","iopub.execute_input":"2024-12-15T19:59:15.460100Z","iopub.status.idle":"2024-12-15T19:59:15.485623Z","shell.execute_reply.started":"2024-12-15T19:59:15.460073Z","shell.execute_reply":"2024-12-15T19:59:15.484858Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                          image_path image_name  age  \\\n0     /kaggle/input/facial-age/face_age/057/6802.png   6802.png   57   \n1     /kaggle/input/facial-age/face_age/057/3702.png   3702.png   57   \n2     /kaggle/input/facial-age/face_age/057/8810.png   8810.png   57   \n3     /kaggle/input/facial-age/face_age/057/6759.png   6759.png   57   \n4     /kaggle/input/facial-age/face_age/057/1846.png   1846.png   57   \n...                                              ...        ...  ...   \n9773  /kaggle/input/facial-age/face_age/085/7733.png   7733.png   85   \n9774  /kaggle/input/facial-age/face_age/085/3123.png   3123.png   85   \n9775  /kaggle/input/facial-age/face_age/085/5302.png   5302.png   85   \n9776  /kaggle/input/facial-age/face_age/085/9632.png   9632.png   85   \n9777  /kaggle/input/facial-age/face_age/085/1710.png   1710.png   85   \n\n     Age_Group   Age_Interval  \n0            3  (49.0, 110.0]  \n1            3  (49.0, 110.0]  \n2            3  (49.0, 110.0]  \n3            3  (49.0, 110.0]  \n4            3  (49.0, 110.0]  \n...        ...            ...  \n9773         3  (49.0, 110.0]  \n9774         3  (49.0, 110.0]  \n9775         3  (49.0, 110.0]  \n9776         3  (49.0, 110.0]  \n9777         3  (49.0, 110.0]  \n\n[9778 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>image_name</th>\n      <th>age</th>\n      <th>Age_Group</th>\n      <th>Age_Interval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/facial-age/face_age/057/6802.png</td>\n      <td>6802.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/facial-age/face_age/057/3702.png</td>\n      <td>3702.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/facial-age/face_age/057/8810.png</td>\n      <td>8810.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/facial-age/face_age/057/6759.png</td>\n      <td>6759.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/facial-age/face_age/057/1846.png</td>\n      <td>1846.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9773</th>\n      <td>/kaggle/input/facial-age/face_age/085/7733.png</td>\n      <td>7733.png</td>\n      <td>85</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>9774</th>\n      <td>/kaggle/input/facial-age/face_age/085/3123.png</td>\n      <td>3123.png</td>\n      <td>85</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>9775</th>\n      <td>/kaggle/input/facial-age/face_age/085/5302.png</td>\n      <td>5302.png</td>\n      <td>85</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>9776</th>\n      <td>/kaggle/input/facial-age/face_age/085/9632.png</td>\n      <td>9632.png</td>\n      <td>85</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>9777</th>\n      <td>/kaggle/input/facial-age/face_age/085/1710.png</td>\n      <td>1710.png</td>\n      <td>85</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>9778 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torchvision.transforms as transforms\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nimport os\n\n\ntransform_pipeline = transforms.Compose([\n    transforms.ToTensor(),                     # Convert image to tensor\n    # transforms.ToPILImage(),                   # Convert OpenCV image (NumPy array) to PIL format\n    transforms.RandomHorizontalFlip(p=0.5),    # 50% chance of horizontal flip\n    transforms.RandomRotation(30),             # Random rotation within Â±30 degrees\n    transforms.Resize((224, 224)),             # Resize to desired input size (224x224 here)\n    # transforms.Normalize([0.5], [0.5])         # Normalize; mean and std of 0.5 for grayscale\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:59:15.486497Z","iopub.execute_input":"2024-12-15T19:59:15.486743Z","iopub.status.idle":"2024-12-15T19:59:15.492777Z","shell.execute_reply.started":"2024-12-15T19:59:15.486716Z","shell.execute_reply":"2024-12-15T19:59:15.491877Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def test_processing(df):\n    brightImages = []\n    smallImages = []\n    dullImages = []\n    \n    blist = []\n    varList = []\n    for idx in range(len(df)):\n        img_path = df.iloc[idx]['image_path']\n        img_path = os.path.join(img_path)\n        label = df.iloc[idx]['Age_Group']  # Age group label for classification\n\n        image = Image.open(img_path)\n        img_array = np.array(image)\n\n        brightness = np.mean(img_array)\n        blist.append(brightness)\n        if brightness < 50 or brightness > 225:  \n            brightImages.append(idx)\n\n        if img_array.shape[0] < 50 or img_array.shape[1] < 50:  # Example minimum size\n            smallImages.append(idx)\n            \n        gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n        variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n        varList.append(variance)\n        if variance < 22.814826998125:\n            dullImages.append(idx)\n            \n    \n    # print(brightImages)\n    # print(smallImages)\n    # print(dullImages)\n    return blist, varList, dullImages\n            \n            \nblist, varlist, dullImages = test_processing(df)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:59:15.493871Z","iopub.execute_input":"2024-12-15T19:59:15.494506Z","iopub.status.idle":"2024-12-15T20:00:21.046765Z","shell.execute_reply.started":"2024-12-15T19:59:15.494455Z","shell.execute_reply":"2024-12-15T20:00:21.045986Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nmodel = models.vgg16(pretrained=None)\n\n\nnum_classes = 5\n\n# model.classifier[6] = nn.Linear(4096, num_classes)\nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, num_classes)\nfeature_extractor = torch.nn.Sequential(*(list(model.children())[:-1]))\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:00:21.047768Z","iopub.execute_input":"2024-12-15T20:00:21.047994Z","iopub.status.idle":"2024-12-15T20:00:23.783764Z","shell.execute_reply.started":"2024-12-15T20:00:21.047972Z","shell.execute_reply":"2024-12-15T20:00:23.783042Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nclass facialAge(Dataset):\n    def __init__(self, df, transform=None, preprocess=None):\n        self.df = df\n        self.transform = transform\n        self.preprocess = preprocess\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['image_path']\n        img_path = os.path.join(img_path)\n        label = self.df.iloc[idx]['Age_Group']  # Age group label for classification\n\n        image = Image.open(img_path)\n        if self.preprocess:\n            image = self.preprocess(image)\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n    \n    \ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=21)\n\ntrain_dataset = facialAge(train_df, transform_pipeline)\ntest_dataset = facialAge(test_df, transform_pipeline)\n\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:00:23.785753Z","iopub.execute_input":"2024-12-15T20:00:23.786017Z","iopub.status.idle":"2024-12-15T20:00:23.805090Z","shell.execute_reply.started":"2024-12-15T20:00:23.785992Z","shell.execute_reply":"2024-12-15T20:00:23.804251Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                          image_path image_name  age  \\\n6508  /kaggle/input/facial-age/face_age/011/3280.png   3280.png   11   \n5595  /kaggle/input/facial-age/face_age/066/4167.png   4167.png   66   \n6565  /kaggle/input/facial-age/face_age/004/8794.png   8794.png    4   \n279   /kaggle/input/facial-age/face_age/053/8225.png   8225.png   53   \n4218  /kaggle/input/facial-age/face_age/034/6673.png   6673.png   34   \n...                                              ...        ...  ...   \n9336  /kaggle/input/facial-age/face_age/038/5783.png   5783.png   38   \n48    /kaggle/input/facial-age/face_age/057/2579.png   2579.png   57   \n8964  /kaggle/input/facial-age/face_age/010/6241.png   6241.png   10   \n5944   /kaggle/input/facial-age/face_age/002/955.png    955.png    2   \n5327  /kaggle/input/facial-age/face_age/008/2218.png   2218.png    8   \n\n     Age_Group   Age_Interval  \n6508         1    (7.0, 25.0]  \n5595         3  (49.0, 110.0]  \n6565         0   (0.999, 7.0]  \n279          3  (49.0, 110.0]  \n4218         2   (25.0, 49.0]  \n...        ...            ...  \n9336         2   (25.0, 49.0]  \n48           3  (49.0, 110.0]  \n8964         1    (7.0, 25.0]  \n5944         0   (0.999, 7.0]  \n5327         1    (7.0, 25.0]  \n\n[7822 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>image_name</th>\n      <th>age</th>\n      <th>Age_Group</th>\n      <th>Age_Interval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6508</th>\n      <td>/kaggle/input/facial-age/face_age/011/3280.png</td>\n      <td>3280.png</td>\n      <td>11</td>\n      <td>1</td>\n      <td>(7.0, 25.0]</td>\n    </tr>\n    <tr>\n      <th>5595</th>\n      <td>/kaggle/input/facial-age/face_age/066/4167.png</td>\n      <td>4167.png</td>\n      <td>66</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>6565</th>\n      <td>/kaggle/input/facial-age/face_age/004/8794.png</td>\n      <td>8794.png</td>\n      <td>4</td>\n      <td>0</td>\n      <td>(0.999, 7.0]</td>\n    </tr>\n    <tr>\n      <th>279</th>\n      <td>/kaggle/input/facial-age/face_age/053/8225.png</td>\n      <td>8225.png</td>\n      <td>53</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>4218</th>\n      <td>/kaggle/input/facial-age/face_age/034/6673.png</td>\n      <td>6673.png</td>\n      <td>34</td>\n      <td>2</td>\n      <td>(25.0, 49.0]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9336</th>\n      <td>/kaggle/input/facial-age/face_age/038/5783.png</td>\n      <td>5783.png</td>\n      <td>38</td>\n      <td>2</td>\n      <td>(25.0, 49.0]</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>/kaggle/input/facial-age/face_age/057/2579.png</td>\n      <td>2579.png</td>\n      <td>57</td>\n      <td>3</td>\n      <td>(49.0, 110.0]</td>\n    </tr>\n    <tr>\n      <th>8964</th>\n      <td>/kaggle/input/facial-age/face_age/010/6241.png</td>\n      <td>6241.png</td>\n      <td>10</td>\n      <td>1</td>\n      <td>(7.0, 25.0]</td>\n    </tr>\n    <tr>\n      <th>5944</th>\n      <td>/kaggle/input/facial-age/face_age/002/955.png</td>\n      <td>955.png</td>\n      <td>2</td>\n      <td>0</td>\n      <td>(0.999, 7.0]</td>\n    </tr>\n    <tr>\n      <th>5327</th>\n      <td>/kaggle/input/facial-age/face_age/008/2218.png</td>\n      <td>2218.png</td>\n      <td>8</td>\n      <td>1</td>\n      <td>(7.0, 25.0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7822 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"Lr 0.001 Batch size 128","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n\n\ndataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n\nimport torch\nimport torch.optim as optim\n\nlearning_rates = [0.001, 0.0001, 0.00001, 0.000001, 0.0000001]\n# Set up loss function and optimizer\ntest_accuracies_across_learning_rates = []\nfor lr in learning_rates:\n\n    model = models.vgg16(pretrained=True)\n\n\n    num_classes = 5\n\n    # model.classifier[6] = nn.Linear(4096, num_classes)\n    num_ftrs = model.classifier[-1].in_features\n    model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n    feature_extractor = torch.nn.Sequential(*(list(model.children())[:-1]))\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    model = model.to(device)\n\n    print(\"Learning Rate: \" + str(lr))\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    num_epochs = 10\n    device ='cuda'\n\n    for epoch in range(num_epochs):\n        total_train = 0\n        correct_train = 0\n        model.train()\n        running_loss = 0.0\n\n        for batch in dataloader:\n            images, labels = batch  # Unpack images and labels\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n            running_loss += loss.item()\n            correct_train += (predicted == labels).sum().item()\n            total_train += labels.size(0)\n\n        train_accuracy = 100 * correct_train / total_train\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}, Train Accuracy: {train_accuracy}\")\n\n\n    model.eval()\n\n    # Initialize variables to track performance\n    correct = 0\n    total = 0\n    test_loss = 0.0\n\n    all_labels = []\n    all_predictions = []\n\n    # Disable gradient computation for testing\n    with torch.no_grad():\n        for batch in test_dataloader:  # Assuming test_dataloader is your DataLoader for the test set\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n\n            # Get predicted class by taking the class with the highest score\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # Append to lists (convert to CPU and detach from computation graph)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    # Calculate average loss and accuracy\n    average_test_loss = test_loss / len(test_dataloader)\n    accuracy = 100 * correct / total\n    test_accuracies_across_learning_rates.append(accuracy)\n    print(f'Test Loss: {average_test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n\n\n\n\n\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n\n\n\n    precision, recall, f1_score, support = precision_recall_fscore_support(all_labels, all_predictions)\n\n    print(\"\\nPrecision per class:\", precision)\n\n    print(\"Recall per class:\", recall)\n\n    print(\"F1-score per class:\", f1_score)\n\n    print(\"Samples per class:\", support)\n\n    print(\"----------------------------------------------------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:00:23.806343Z","iopub.execute_input":"2024-12-15T20:00:23.806763Z","iopub.status.idle":"2024-12-15T22:32:27.388281Z","shell.execute_reply.started":"2024-12-15T20:00:23.806727Z","shell.execute_reply":"2024-12-15T22:32:27.387434Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:02<00:00, 197MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Learning Rate: 0.001\nEpoch [1/10], Loss: 1.5841, Train Accuracy: 26.208130912810024\nEpoch [2/10], Loss: 1.2933, Train Accuracy: 37.138839171567376\nEpoch [3/10], Loss: 1.0801, Train Accuracy: 51.24009204806955\nEpoch [4/10], Loss: 0.9747, Train Accuracy: 57.47890565072871\nEpoch [5/10], Loss: 0.9229, Train Accuracy: 58.25875735106111\nEpoch [6/10], Loss: 0.8762, Train Accuracy: 61.761697775504985\nEpoch [7/10], Loss: 0.8695, Train Accuracy: 61.902326770646894\nEpoch [8/10], Loss: 0.8055, Train Accuracy: 64.44643313730504\nEpoch [9/10], Loss: 0.7900, Train Accuracy: 65.18793147532601\nEpoch [10/10], Loss: 0.7593, Train Accuracy: 66.83712605471746\nTest Loss: 0.7179, Accuracy: 69.07%\n\nPrecision per class: [0.86148008 0.59356137 0.58823529 0.68458781]\nRecall per class: [0.88326848 0.61076605 0.45548654 0.80252101]\nF1-score per class: [0.87223823 0.60204082 0.5134189  0.73887814]\nSamples per class: [514 483 483 476]\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Learning Rate: 0.0001\nEpoch [1/10], Loss: 0.9096, Train Accuracy: 60.2019943748402\nEpoch [2/10], Loss: 0.6098, Train Accuracy: 73.9580669905395\nEpoch [3/10], Loss: 0.5400, Train Accuracy: 77.24367169521861\nEpoch [4/10], Loss: 0.4920, Train Accuracy: 79.16133981079008\nEpoch [5/10], Loss: 0.4764, Train Accuracy: 79.9667604193301\nEpoch [6/10], Loss: 0.4149, Train Accuracy: 82.65149578113015\nEpoch [7/10], Loss: 0.4039, Train Accuracy: 83.30350294042444\nEpoch [8/10], Loss: 0.3506, Train Accuracy: 85.61748913321401\nEpoch [9/10], Loss: 0.3192, Train Accuracy: 86.84479672717976\nEpoch [10/10], Loss: 0.2861, Train Accuracy: 88.0848887752493\nTest Loss: 0.5160, Accuracy: 79.24%\n\nPrecision per class: [0.97840173 0.74007937 0.62950257 0.87931034]\nRecall per class: [0.88132296 0.77225673 0.75983437 0.75      ]\nF1-score per class: [0.92732856 0.75582573 0.68855535 0.80952381]\nSamples per class: [514 483 483 476]\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Learning Rate: 1e-05\nEpoch [1/10], Loss: 1.2040, Train Accuracy: 45.525441063666584\nEpoch [2/10], Loss: 0.8445, Train Accuracy: 63.51316798772692\nEpoch [3/10], Loss: 0.7327, Train Accuracy: 69.31731015085656\nEpoch [4/10], Loss: 0.6761, Train Accuracy: 71.50345180260803\nEpoch [5/10], Loss: 0.6423, Train Accuracy: 73.11429301968806\nEpoch [6/10], Loss: 0.6011, Train Accuracy: 74.80184096139095\nEpoch [7/10], Loss: 0.5886, Train Accuracy: 75.10866785988239\nEpoch [8/10], Loss: 0.5603, Train Accuracy: 76.52774226540527\nEpoch [9/10], Loss: 0.5261, Train Accuracy: 77.8828944004091\nEpoch [10/10], Loss: 0.5100, Train Accuracy: 78.58603937611863\nTest Loss: 0.5415, Accuracy: 76.89%\n\nPrecision per class: [0.89980732 0.68809074 0.63829787 0.87723785]\nRecall per class: [0.90856031 0.75362319 0.68322981 0.72058824]\nF1-score per class: [0.90416263 0.71936759 0.66       0.79123414]\nSamples per class: [514 483 483 476]\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Learning Rate: 1e-06\nEpoch [1/10], Loss: 1.5857, Train Accuracy: 25.632830478138583\nEpoch [2/10], Loss: 1.4276, Train Accuracy: 34.760930708258755\nEpoch [3/10], Loss: 1.3132, Train Accuracy: 41.3577090258246\nEpoch [4/10], Loss: 1.2068, Train Accuracy: 47.32804909230376\nEpoch [5/10], Loss: 1.1123, Train Accuracy: 52.17335719764766\nEpoch [6/10], Loss: 1.0541, Train Accuracy: 54.15494758373818\nEpoch [7/10], Loss: 0.9929, Train Accuracy: 57.018665302991565\nEpoch [8/10], Loss: 0.9638, Train Accuracy: 58.987471234978265\nEpoch [9/10], Loss: 0.9210, Train Accuracy: 60.3426233699821\nEpoch [10/10], Loss: 0.8982, Train Accuracy: 61.00741498338021\nTest Loss: 0.8160, Accuracy: 64.88%\n\nPrecision per class: [0.77333333 0.57112527 0.51287129 0.73626374]\nRecall per class: [0.78988327 0.55693582 0.53623188 0.70378151]\nF1-score per class: [0.78152069 0.5639413  0.5242915  0.71965628]\nSamples per class: [514 483 483 476]\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Learning Rate: 1e-07\nEpoch [1/10], Loss: 1.6462, Train Accuracy: 22.296087957044236\nEpoch [2/10], Loss: 1.6170, Train Accuracy: 23.843006903605215\nEpoch [3/10], Loss: 1.5936, Train Accuracy: 24.622858603937612\nEpoch [4/10], Loss: 1.5739, Train Accuracy: 25.28765021733572\nEpoch [5/10], Loss: 1.5560, Train Accuracy: 27.831756583993865\nEpoch [6/10], Loss: 1.5334, Train Accuracy: 27.767834313474815\nEpoch [7/10], Loss: 1.5153, Train Accuracy: 29.059064177959602\nEpoch [8/10], Loss: 1.4961, Train Accuracy: 30.28637177192534\nEpoch [9/10], Loss: 1.4845, Train Accuracy: 31.33469700843774\nEpoch [10/10], Loss: 1.4694, Train Accuracy: 31.654308361032985\nTest Loss: 1.4196, Accuracy: 39.16%\n\nPrecision per class: [0.54054054 0.32817337 0.29806259 0.41580042]\nRecall per class: [0.50583658 0.2194617  0.41407867 0.42016807]\nF1-score per class: [0.52261307 0.2630273  0.34662045 0.41797283]\nSamples per class: [514 483 483 476]\n----------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8}]}